{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = sys.argv[1]\n",
    "import torch\n",
    "from deepspeech.train import finetune#, train_new, continue_training\n",
    "from audio.dataset_factory import DatasetFactory\n",
    "from sklearn.model_selection import KFold\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Success\")\n",
    "    print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The WER and CER for every sentence in the validation set becomes saved in ./Scores/allScores.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint model /home/user/.danspeech/models/DanSpeechPrimary.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 136\n",
      "Loading dataset...\n",
      "Length of dataset: 64\n",
      "\n",
      "DeepSpeech(\n",
      "  (conv): MaskConv(\n",
      "    (seq_module): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "      (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "      (6): Conv2d(32, 96, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (rnns): Sequential(\n",
      "    (0): BatchRNN(\n",
      "      (rnn): GRU(2016, 1200, bidirectional=True)\n",
      "    )\n",
      "    (1): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (2): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (3): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (4): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (5): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (6): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (7): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (8): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): SequenceWise (\n",
      "    Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Linear(in_features=1200, out_features=33, bias=False)\n",
      "    ))\n",
      "  )\n",
      "  (inference_softmax): InferenceBatchSoftmax()\n",
      ")\n",
      "Initializations complete, starting training pass on model: model_name \n",
      "\n",
      "Number of parameters: 162547088 \n",
      "\n",
      "started training epoch 1\n",
      "Successfully exited training and stopped all processes.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'best_transcript' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/DanSpeech-AdvancedMachineLearning/AML Cleaned/deepspeech/train.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(model_id, training_set, validation_set, root_dir, in_memory, epochs, stored_model, model_save_dir, tensorboard_log_dir, augmented_training, batch_size, num_workers, cuda, lr, momentum, weight_decay, max_norm, context, continue_train, finetune, train_new, num_freeze_layers, rnn_type, conv_layers, rnn_hidden_layers, rnn_hidden_size, bidirectional, distributed, gpu_rank, dist_backend, rank, dist_url, world_size, augment_w_specaug, augmentation_list, augment_parameters, augment_prob_dir, score_ID)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/danspeech_train/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/danspeech_train/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-baa6d5233200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                  }\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m f_train_gpu(model_name = model_name, k_folds = None,epochs=15,\n\u001b[0m\u001b[1;32m     26\u001b[0m             root_dir=root_dir,augmentation_list=augmentation_list,augment_parameters=augment_parameters)\n",
      "\u001b[0;32m~/DanSpeech-AdvancedMachineLearning/AML Cleaned/train_gpu.py\u001b[0m in \u001b[0;36mf_train_gpu\u001b[0;34m(model_name, augmentation_list, k_folds, epochs, root_dir, augment_parameters, augment_prob_dir)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.67\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         finetune(model_id=model_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                  \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                  \u001b[0mtraining_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DanSpeech-AdvancedMachineLearning/AML Cleaned/deepspeech/train.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(model_id, training_set, validation_set, root_dir, in_memory, epochs, stored_model, model_save_dir, tensorboard_log_dir, num_freeze_layers, augment_w_specaug, augmentation_list, augment_parameters, lr, score_ID, **args)\u001b[0m\n\u001b[1;32m    511\u001b[0m              **args):\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m     _train_model(model_id, training_set=training_set, validation_set=validation_set, root_dir=root_dir,\n\u001b[0m\u001b[1;32m    514\u001b[0m                  \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstored_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstored_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                  \u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_log_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorboard_log_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinetune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DanSpeech-AdvancedMachineLearning/AML Cleaned/deepspeech/train.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(model_id, training_set, validation_set, root_dir, in_memory, epochs, stored_model, model_save_dir, tensorboard_log_dir, augmented_training, batch_size, num_workers, cuda, lr, momentum, weight_decay, max_norm, context, continue_train, finetune, train_new, num_freeze_layers, rnn_type, conv_layers, rnn_hidden_layers, rnn_hidden_size, bidirectional, distributed, gpu_rank, dist_backend, rank, dist_url, world_size, augment_w_specaug, augmentation_list, augment_parameters, augment_prob_dir, score_ID)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Successfully exited training and stopped all processes.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mcsvSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_transcript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'best_transcript' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Finetuning DanSpeechPrimary with no augmentations and with all augmentations on clean and noisy training \n",
    "# and validation set \n",
    "# \n",
    "\n",
    "# 15 epochs\n",
    "\n",
    "from train_gpu import f_train_gpu\n",
    "\n",
    "augmentation_list=[\"speed_perturb\",\"room_reverb\",\"volume_perturb\",\"add_wn\",\"shift_perturb\",\"spec_augment\"]\n",
    "#augmentation_list = []\n",
    "\n",
    "model_name = \"model_name\"\n",
    "#root_dir = '../Data/Tue_Noise_test_split/'# <- Noisy dataset\n",
    "root_dir = '../Data/Tue_test_split/' # <- Clean dataset\n",
    "\n",
    "augment_parameters = {'speed_perturb'  : [0.9,1.1],\n",
    "                  'shift_perturb'  : [-50,50],\n",
    "                  'room_reverb'    : [0, 0.4, 3.0, 0, 5, 3.0, 0, 5, 3.0, 0, 5, 0.5, 0.5, 0.5],# <- original\n",
    "                  'volume_perturb' : [5,30],\n",
    "                  'add_wn'         : [0.5,1.8],\n",
    "                  'tempo_perturb'  : [-0.25,0.25],\n",
    "                  'spec_augment'   : [80, 27, 50, 1, 1] # <- original\n",
    "                 }\n",
    "\n",
    "f_train_gpu(model_name = model_name, k_folds = None,epochs=15,\n",
    "            root_dir=root_dir,augmentation_list=augmentation_list,augment_parameters=augment_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The WER and CER for every sentence in the test set becomes saved in ./Scores/allScoresTest.csv \n",
    "\n",
    "* Results and confidence intervals for baseline results can be seen in CI_for_Baseline Results.ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_gpu import test_GPU \n",
    "\n",
    "test_GPU(\n",
    "#         \"/home/user/.danspeech/models/DanSpeechPrimary.pth\",\n",
    "         \"/home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/noisy_all_augs.pth\", \n",
    "#         \"/home/user/DanSpeech-AdvancedMachineLearning/Data/Tue_test_split\") \n",
    "          \"/home/user/DanSpeech-AdvancedMachineLearning/Data/Tue_Noise_test_split\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing cross validation for forward and backward selection\n",
    "\n",
    "Script performs first round of either forward or backward selection. \n",
    "\n",
    "* Results and confidence interval of forward and backward selection can be seen in Forward_&_Backward_Confidence_Interval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "1\n",
      "Loading checkpoint model /home/user/.danspeech/models/DanSpeechPrimary.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/deepspeech/train.py:103: NoLoggingDirSpecified: You did not specify a directory for logging training process. Training process will not be logged.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Length of dataset: 133\n",
      "Loading dataset...\n",
      "Length of dataset: 67\n",
      "\n",
      "DeepSpeech(\n",
      "  (conv): MaskConv(\n",
      "    (seq_module): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "      (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "      (6): Conv2d(32, 96, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (rnns): Sequential(\n",
      "    (0): BatchRNN(\n",
      "      (rnn): GRU(2016, 1200, bidirectional=True)\n",
      "    )\n",
      "    (1): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (2): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (3): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (4): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (5): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (6): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (7): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (8): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): SequenceWise (\n",
      "    Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Linear(in_features=1200, out_features=33, bias=False)\n",
      "    ))\n",
      "  )\n",
      "  (inference_softmax): InferenceBatchSoftmax()\n",
      ")\n",
      "Initializations complete, starting training pass on model: AblationNoise_Baseline \n",
      "\n",
      "Number of parameters: 162547088 \n",
      "\n",
      "started training epoch 1\n",
      "Training Summary Epoch: [1]\tTime taken (s): 49\tAverage Loss 34.391\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:12<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [1]\tAverage WER 34.752\tAverage CER 9.248\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/AblationNoise_Baseline.pth\n",
      "started training epoch 2\n",
      "Successfully exited training and stopped all processes.\n",
      "Saving all scores into csv...\n",
      "Loading checkpoint model /home/user/.danspeech/models/DanSpeechPrimary.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 133\n",
      "Loading dataset...\n",
      "Length of dataset: 67\n",
      "\n",
      "DeepSpeech(\n",
      "  (conv): MaskConv(\n",
      "    (seq_module): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "      (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "      (6): Conv2d(32, 96, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (rnns): Sequential(\n",
      "    (0): BatchRNN(\n",
      "      (rnn): GRU(2016, 1200, bidirectional=True)\n",
      "    )\n",
      "    (1): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (2): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (3): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (4): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (5): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (6): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (7): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (8): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): SequenceWise (\n",
      "    Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Linear(in_features=1200, out_features=33, bias=False)\n",
      "    ))\n",
      "  )\n",
      "  (inference_softmax): InferenceBatchSoftmax()\n",
      ")\n",
      "Initializations complete, starting training pass on model: AblationNoise_Baseline \n",
      "\n",
      "Number of parameters: 162547088 \n",
      "\n",
      "started training epoch 1\n",
      "Training Summary Epoch: [1]\tTime taken (s): 54\tAverage Loss 38.105\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [1]\tAverage WER 25.921\tAverage CER 8.198\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/AblationNoise_Baseline.pth\n",
      "started training epoch 2\n",
      "Training Summary Epoch: [2]\tTime taken (s): 53\tAverage Loss 11.099\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [2]\tAverage WER 23.690\tAverage CER 7.643\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/AblationNoise_Baseline.pth\n",
      "started training epoch 3\n",
      "Training Summary Epoch: [3]\tTime taken (s): 54\tAverage Loss 3.609\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [3]\tAverage WER 24.145\tAverage CER 7.375\t\n",
      "started training epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [4]\tTime taken (s): 53\tAverage Loss 1.555\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [4]\tAverage WER 24.218\tAverage CER 7.486\t\n",
      "started training epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [5]\tTime taken (s): 54\tAverage Loss 0.816\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [5]\tAverage WER 22.631\tAverage CER 7.102\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/AblationNoise_Baseline.pth\n",
      "started training epoch 6\n",
      "Training Summary Epoch: [6]\tTime taken (s): 53\tAverage Loss 0.346\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [6]\tAverage WER 22.221\tAverage CER 7.129\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/AblationNoise_Baseline.pth\n",
      "started training epoch 7\n",
      "Training Summary Epoch: [7]\tTime taken (s): 52\tAverage Loss 0.304\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [7]\tAverage WER 22.182\tAverage CER 6.937\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/AblationNoise_Baseline.pth\n",
      "started training epoch 8\n",
      "Training Summary Epoch: [8]\tTime taken (s): 54\tAverage Loss 0.140\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [8]\tAverage WER 22.186\tAverage CER 6.809\t\n",
      "started training epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [9]\tTime taken (s): 54\tAverage Loss 0.195\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [9]\tAverage WER 21.773\tAverage CER 6.846\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/AblationNoise_Baseline.pth\n",
      "started training epoch 10\n",
      "Training Summary Epoch: [10]\tTime taken (s): 54\tAverage Loss 0.087\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [10]\tAverage WER 21.487\tAverage CER 6.829\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/AblationNoise_Baseline.pth\n"
     ]
    }
   ],
   "source": [
    "from train_gpu import f_train_gpu\n",
    "\n",
    "feat_sel_method = \"forward\" # \"forward\" or \"backward\"\n",
    "k_folds = 3\n",
    "epochs = 15\n",
    "\n",
    "root_dir = '../Data/Tue_test_split/'\n",
    "\n",
    "augment_parameters = {'speed_perturb'  : [0.9,1.1],\n",
    "                  'shift_perturb'  : [-50,50],\n",
    "                  'room_reverb'    : [0, 0.4, 3.0, 0, 5, 3.0, 0, 5, 3.0, 0, 5, 0.5, 0.5, 0.5],# <- original\n",
    "                  'volume_perturb' : [5,30],\n",
    "                  'add_wn'         : [0.5,1.8],\n",
    "                  'tempo_perturb'  : [-0.25,0.25],\n",
    "                  'spec_augment'   : [80, 27, 50, 1, 1] # <- original\n",
    "                 }\n",
    "\n",
    "\n",
    "if feat_sel_method == \"forward\":\n",
    "    \n",
    "\n",
    "    augmentation_list=[\"speed_perturb\",\"tempo_perturb\",\"room_reverb\",\"volume_perturb\",\"add_wn\",\"shift_perturb\",\"spec_augment\"]\n",
    "\n",
    "    model_name = \"AblationNoise_Baseline\"    \n",
    "    f_train_gpu(model_name = model_name, k_folds = k_folds,root_dir=root_dir,\n",
    "                augment_parameters=augment_parameters)\n",
    "\n",
    "    for i in range(len(augmentation_list)):\n",
    "\n",
    "        aug_method = []\n",
    "        aug_method.append(augmentation_list[i])\n",
    "\n",
    "        model_name = \"AblationClean_only_\" + augmentation_list[i]\n",
    "        f_train_gpu(model_name, aug_method, k_folds,root_dir=root_dir,\n",
    "                    augment_parameters=augment_parameters)\n",
    "\n",
    "\n",
    "if feat_sel_method == \"backward\":\n",
    "\n",
    "    augmentation_list=[\"speed_perturb\",\"tempo_perturb\",\"room_reverb\",\"volume_perturb\",\"add_wn\",\"shift_perturb\",\"spec_augment\"]\n",
    "\n",
    "\n",
    "    model_name = \"AblationClean_Baseline_allAugs\"\n",
    "    f_train_gpu(model_name,augmentation_list,k_folds,root_dir=root_dir,\n",
    "                augment_parameters=augment_parameters)\n",
    "\n",
    "    for i in range(len(augmentation_list)):\n",
    "        aug_temp = augmentation_list.copy()\n",
    "        aug_temp.pop(i)\n",
    "\n",
    "        model_name = \"AblationClean_no_\" + augmentation_list[i]\n",
    "\n",
    "        f_train_gpu(model_name, aug_temp, k_folds,root_dir=root_dir,\n",
    "                    augment_parameters=augment_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of models on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_gpu import test_GPU \n",
    "\n",
    "test_GPU(\"/home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/noisy_all_augs.pth\", \n",
    "#         \"/home/user/DanSpeech-AdvancedMachineLearning/Data/Tue_test_split\") \n",
    "          \"/home/user/DanSpeech-AdvancedMachineLearning/Data/Tue_Noise_test_split\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Script for hyper-parameter tuning is called train_gpu_hyperparameter_tuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "1\n",
      "Loading checkpoint model /home/user/.danspeech/models/DanSpeechPrimary.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/deepspeech/train.py:103: NoLoggingDirSpecified: You did not specify a directory for logging training process. Training process will not be logged.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Length of dataset: 136\n",
      "Loading dataset...\n",
      "Length of dataset: 64\n",
      "\n",
      "DeepSpeech(\n",
      "  (conv): MaskConv(\n",
      "    (seq_module): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "      (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "      (6): Conv2d(32, 96, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (rnns): Sequential(\n",
      "    (0): BatchRNN(\n",
      "      (rnn): GRU(2016, 1200, bidirectional=True)\n",
      "    )\n",
      "    (1): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (2): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (3): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (4): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (5): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (6): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (7): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "    (8): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(1200, 1200, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): SequenceWise (\n",
      "    Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Linear(in_features=1200, out_features=33, bias=False)\n",
      "    ))\n",
      "  )\n",
      "  (inference_softmax): InferenceBatchSoftmax()\n",
      ")\n",
      "Initializations complete, starting training pass on model: Clean_original_hyper-parameters.pth \n",
      "\n",
      "Number of parameters: 162547088 \n",
      "\n",
      "started training epoch 1\n",
      "Training Summary Epoch: [1]\tTime taken (s): 53\tAverage Loss 32.792\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [1]\tAverage WER 28.655\tAverage CER 8.741\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/Clean_original_hyper-parameters.pth.pth\n",
      "started training epoch 2\n",
      "Training Summary Epoch: [2]\tTime taken (s): 53\tAverage Loss 14.273\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [2]\tAverage WER 27.457\tAverage CER 8.514\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/Clean_original_hyper-parameters.pth.pth\n",
      "started training epoch 3\n",
      "Training Summary Epoch: [3]\tTime taken (s): 53\tAverage Loss 7.213\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [3]\tAverage WER 28.799\tAverage CER 8.100\t\n",
      "started training epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [4]\tTime taken (s): 53\tAverage Loss 5.125\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [4]\tAverage WER 28.561\tAverage CER 7.850\t\n",
      "started training epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [5]\tTime taken (s): 54\tAverage Loss 2.614\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [5]\tAverage WER 27.353\tAverage CER 7.701\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/Clean_original_hyper-parameters.pth.pth\n",
      "started training epoch 6\n",
      "Training Summary Epoch: [6]\tTime taken (s): 54\tAverage Loss 1.813\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [6]\tAverage WER 26.552\tAverage CER 7.748\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/Clean_original_hyper-parameters.pth.pth\n",
      "started training epoch 7\n",
      "Training Summary Epoch: [7]\tTime taken (s): 54\tAverage Loss 1.189\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [7]\tAverage WER 25.971\tAverage CER 7.529\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/Clean_original_hyper-parameters.pth.pth\n",
      "started training epoch 8\n",
      "Training Summary Epoch: [8]\tTime taken (s): 55\tAverage Loss 2.920\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [8]\tAverage WER 25.284\tAverage CER 7.470\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/Clean_original_hyper-parameters.pth.pth\n",
      "started training epoch 9\n",
      "Training Summary Epoch: [9]\tTime taken (s): 53\tAverage Loss 2.278\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [9]\tAverage WER 26.754\tAverage CER 7.897\t\n",
      "started training epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [10]\tTime taken (s): 53\tAverage Loss 2.090\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [10]\tAverage WER 26.486\tAverage CER 7.702\t\n",
      "started training epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [11]\tTime taken (s): 54\tAverage Loss 2.111\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [11]\tAverage WER 26.191\tAverage CER 7.634\t\n",
      "started training epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [12]\tTime taken (s): 53\tAverage Loss 0.743\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [12]\tAverage WER 25.887\tAverage CER 7.570\t\n",
      "started training epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [13]\tTime taken (s): 54\tAverage Loss 1.017\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [13]\tAverage WER 24.975\tAverage CER 7.389\t\n",
      "Found better validated model, saving to /home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/Clean_original_hyper-parameters.pth.pth\n",
      "started training epoch 14\n",
      "Training Summary Epoch: [14]\tTime taken (s): 54\tAverage Loss 1.765\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [14]\tAverage WER 25.629\tAverage CER 7.346\t\n",
      "started training epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Summary Epoch: [15]\tTime taken (s): 53\tAverage Loss 0.649\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:10<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [15]\tAverage WER 25.430\tAverage CER 7.362\t\n",
      "Scores saved...\n",
      "Saving all scores into csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from train_gpu import f_train_gpu\n",
    "\n",
    "\n",
    "model_name = \"Clean_original_hyper-parameters.pth\"\n",
    "\n",
    "#root_dir = '../Data/Tue_Noise_test_split/'# <- Noisy dataset\n",
    "root_dir = '../Data/Tue_test_split/' # <- Clean dataset\n",
    "\n",
    "\n",
    "#augmentation_list=[\"room_reverb\",\"spec_augment\"] # <- Candidate 1: policy for noisy\n",
    "#augmentation_list = [\"room_reverb\"] # <- Candidate 2: policy for noisy\n",
    "#augmentation_list = [\"spec_augment\"] # <- Candidate 3: policy for noisy\n",
    "augmentation_list = [\"room_reverb\"] # <- policy for clean\n",
    "\n",
    "\n",
    "augment_parameters = {'speed_perturb'  : [0.9,1.1],\n",
    "                  'shift_perturb'  : [-50,50],\n",
    "                  'room_reverb'    : [0, 0.4, 3.0, 0, 5, 3.0, 0, 5, 3.0, 0, 5, 0.5, 0.5, 0.5],# <- original\n",
    "#                  'room_reverb'    : [0.13, 0.36, 3.19, 0, 5, 7.49, 0, 5, 7.08, 0, 0.5, 0.5, 0.5, 0.5], # <- best for noisy\n",
    "#                  'room_reverb'    : [0.1, 0.52, 5.9, 0, 5, 3.4, 0, 5, 5.92, 0, 0.5, 0.5, 0.5, 0.5], # <- best for clean\n",
    "                  'volume_perturb' : [5,30],\n",
    "                  'add_wn'         : [0.5,1.8],\n",
    "                  'tempo_perturb'  : [-0.25,0.25],\n",
    "                  'spec_augment'   : [80, 27, 50, 1, 1] # <- original\n",
    "#                  'spec_augment'   : [61.2, 31.5, 48.47, 1, 1] # <- best for noisy\n",
    "                 }\n",
    "\n",
    "\n",
    "# Out comment everything if the original DanSpeech parameters are used\n",
    "#augment_prob_dir = {\n",
    "#                    'room_reverb': 0.64,    # <- Probability for clean\n",
    "#                    'room_reverb': 0.89, # <- Probability for noisy\n",
    "#                    'spec_augment': 0.05 # <- Probability for noisy\n",
    "#                   } \n",
    "\n",
    "f_train_gpu(model_name = model_name, k_folds = None,epochs=15,\n",
    "            root_dir=root_dir,\n",
    "            augmentation_list=augmentation_list,\n",
    "            augment_parameters=augment_parameters,\n",
    "            \n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of final models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Confidence intervals for final models can be found at Final_Models_scores.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "1\n",
      "Greedy:\n",
      "Clean_original_hyper-parameters.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 21.127\tAverage CER 5.972\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "3-gram language model:\n",
      "Clean_original_hyper-parameters.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 12.676\tAverage CER 5.268\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "5-gram language model:\n",
      "Clean_original_hyper-parameters.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 12.500\tAverage CER 5.237\t\n",
      "Saving all test scores into csv...\n",
      "Greedy:\n",
      "Clean_original_hyper-parameters.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 54.391\tAverage CER 22.009\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "3-gram language model:\n",
      "Clean_original_hyper-parameters.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 43.343\tAverage CER 22.894\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "5-gram language model:\n",
      "Clean_original_hyper-parameters.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 43.626\tAverage CER 22.795\t\n",
      "Saving all test scores into csv...\n",
      "Greedy:\n",
      "Noisy_original_hyper-parameters_both_augs.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 25.000\tAverage CER 7.565\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "3-gram language model:\n",
      "Noisy_original_hyper-parameters_both_augs.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 18.486\tAverage CER 7.688\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "5-gram language model:\n",
      "Noisy_original_hyper-parameters_both_augs.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 18.486\tAverage CER 7.688\t\n",
      "Saving all test scores into csv...\n",
      "Greedy:\n",
      "Noisy_original_hyper-parameters_both_augs.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 48.867\tAverage CER 19.504\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "3-gram language model:\n",
      "Noisy_original_hyper-parameters_both_augs.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 44.193\tAverage CER 22.230\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "5-gram language model:\n",
      "Noisy_original_hyper-parameters_both_augs.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 44.618\tAverage CER 22.501\t\n",
      "Saving all test scores into csv...\n",
      "Greedy:\n",
      "Noisy_original_hyper-parameters_only_room_reverb.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 22.535\tAverage CER 6.799\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "3-gram language model:\n",
      "Noisy_original_hyper-parameters_only_room_reverb.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 14.261\tAverage CER 6.034\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "5-gram language model:\n",
      "Noisy_original_hyper-parameters_only_room_reverb.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 13.908\tAverage CER 5.972\t\n",
      "Saving all test scores into csv...\n",
      "Greedy:\n",
      "Noisy_original_hyper-parameters_only_room_reverb.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 47.025\tAverage CER 19.037\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "3-gram language model:\n",
      "Noisy_original_hyper-parameters_only_room_reverb.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 37.960\tAverage CER 19.184\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "5-gram language model:\n",
      "Noisy_original_hyper-parameters_only_room_reverb.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 38.527\tAverage CER 19.234\t\n",
      "Saving all test scores into csv...\n",
      "Greedy:\n",
      "Noisy_original_hyper-parameters_only_spec_augment.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 24.648\tAverage CER 7.259\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "3-gram language model:\n",
      "Noisy_original_hyper-parameters_only_spec_augment.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 16.549\tAverage CER 6.769\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "5-gram language model:\n",
      "Noisy_original_hyper-parameters_only_spec_augment.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 16.901\tAverage CER 7.075\t\n",
      "Saving all test scores into csv...\n",
      "Greedy:\n",
      "Noisy_original_hyper-parameters_only_spec_augment.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 50.850\tAverage CER 20.290\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "3-gram language model:\n",
      "Noisy_original_hyper-parameters_only_spec_augment.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 45.467\tAverage CER 22.132\t\n",
      "Saving all test scores into csv...\n",
      "\n",
      "\n",
      "5-gram language model:\n",
      "Noisy_original_hyper-parameters_only_spec_augment.pth\n",
      "Loading dataset...\n",
      "Length of dataset: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary \tAverage WER 45.609\tAverage CER 22.058\t\n",
      "Saving all test scores into csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from test_gpu import test_GPU\n",
    "\n",
    "models = [\n",
    "          \"Clean_original_hyper-parameters.pth\",\n",
    "          \"Noisy_original_hyper-parameters_both_augs.pth\",\n",
    "          \"Noisy_original_hyper-parameters_only_room_reverb.pth\",\n",
    "          \"Noisy_original_hyper-parameters_only_spec_augment.pth\",\n",
    "         ]\n",
    "\n",
    "for m in models: \n",
    "    \n",
    "    model_path = f\"/home/user/DanSpeech-AdvancedMachineLearning/AML Cleaned/model_save_dir/{m}\"\n",
    "\n",
    "    test_GPU(model_path = model_path, \n",
    "          data_path=\"/home/user/DanSpeech-AdvancedMachineLearning/Data/Tue_test_split\")\n",
    "\n",
    "    test_GPU(model_path = model_path, \n",
    "             data_path =\"/home/user/DanSpeech-AdvancedMachineLearning/Data/Tue_Noise_test_split\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
